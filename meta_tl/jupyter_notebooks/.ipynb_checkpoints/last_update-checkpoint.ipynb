{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4ad64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "\n",
    "# Converts weight values from various units to pounds\n",
    "def convert_weight_to_pounds(weight_str):\n",
    "    if pd.isna(weight_str):\n",
    "        return weight_str  # Return NaN as is\n",
    "    if 'oz' in weight_str.lower():\n",
    "        ounces = float(weight_str.split(' ')[0])  # Extract the number of ounces\n",
    "        pounds = ounces / 16.0  # Convert ounces to pounds (1 pound = 16 ounces)\n",
    "        return f'{pounds:.2f}'\n",
    "    elif 'lbs' in weight_str.lower():\n",
    "        return weight_str.split(' ')[0]  # Remove \"lbs\" suffix\n",
    "    else:\n",
    "        return 'Invalid'\n",
    "\n",
    "\n",
    "# Consolidates weight values across multiple columns into a single column\n",
    "def consolidate_weight_columns(row):\n",
    "    for col in row.index:\n",
    "        if not pd.isna(row[col]) and 'weight' in col:\n",
    "            return row[col]\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# Cleans numeric columns by removing non-numeric characters and converting to numeric type\n",
    "def clean_numeric_columns(df, columns):\n",
    "    df[columns] = df[columns].replace(r'[^0-9.]', '', regex=True).apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Consolidates dimension values across multiple columns into a single column\n",
    "def consolidate_dimension_columns(row):\n",
    "    for col in row.index:\n",
    "        if not pd.isna(row[col]) and 'dimensions' in col:\n",
    "            return row[col]\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# Converts various weight units to grams\n",
    "def convert_weight_to_grams(weight_str):\n",
    "    if pd.isna(weight_str):\n",
    "        return weight_str  # Return NaN as is\n",
    "    if 'oz' in weight_str.lower():\n",
    "        ounces = float(weight_str.split(' ')[0])\n",
    "        return ounces * 28.3495  # Convert ounces to grams\n",
    "    elif 'lb' in weight_str.lower() or 'pounds' in weight_str.lower():\n",
    "        pounds = float(weight_str.split(' ')[0])\n",
    "        return pounds * 453.592  # Convert pounds to grams\n",
    "    elif 'kg' in weight_str.lower():\n",
    "        kilograms = float(weight_str.split(' ')[0])\n",
    "        return kilograms * 1000  # Convert kilograms to grams\n",
    "    else:\n",
    "        return re.sub(r'gr', '', weight_str).replace('\"', '').replace('\\\\', '')\n",
    "\n",
    "\n",
    "# Extracts specific dimensions (width, height, depth) from a dimension string\n",
    "def extract_dimension(dimensions_str, dimension,split_num):\n",
    "    if pd.isna(dimensions_str):\n",
    "        return dimensions_str\n",
    "    splitt_array = dimensions_str.replace('â€³', '').strip().split('x')\n",
    "    if 'cm' in dimensions_str:\n",
    "        if dimension == 'width':\n",
    "            width = splitt_array[split_num]\n",
    "            return width\n",
    "        if dimension == 'height':\n",
    "            width = splitt_array[split_num]\n",
    "            return width\n",
    "        if dimension == 'depth':\n",
    "            width = splitt_array[split_num]\n",
    "            return width\n",
    "    elif 'mm' in dimensions_str:\n",
    "        if len(splitt_array) >= 3:\n",
    "             if dimension == 'width':\n",
    "                width = splitt_array[split_num] + ' mm'\n",
    "                return width\n",
    "             if dimension == 'height':\n",
    "                width = splitt_array[split_num] + ' mm'\n",
    "                return width\n",
    "             if dimension == 'depth':\n",
    "                width = splitt_array[split_num] + ' mm'\n",
    "                return width\n",
    "    else:\n",
    "        return 'Invalid'\n",
    "\n",
    "\n",
    "\n",
    "# Extracts resolution dimensions (width or height) from a resolution string\n",
    "def extract_resolution(resolution_str, dimension):\n",
    "    if pd.isna(resolution_str):\n",
    "        return resolution_str\n",
    "    dimensions = resolution_str.split('x')\n",
    "    return dimensions[0] if dimension == 'x' else dimensions[1]\n",
    "\n",
    "\n",
    "# Converts dimension values from millimeters to centimeters\n",
    "def convert_mm_to_cm(dimension_str):\n",
    "    dimension_str = str(dimension_str)\n",
    "    if pd.isna(dimension_str):\n",
    "        return dimension_str\n",
    "    elif 'cm' in dimension_str:\n",
    "        dimension_str_splitted = dimension_str.split(' ')\n",
    "        return dimension_str_splitted[0]\n",
    "    elif not any(char.isdigit() for char in dimension_str):\n",
    "        return 0\n",
    "    else:\n",
    "        numeric_part = re.search(r'\\d+', dimension_str)\n",
    "\n",
    "        return float(numeric_part.group()) / 10.0\n",
    "\n",
    "\n",
    "# Converts various dimension units to centimeters\n",
    "def convert_dimension_to_cm(dim_str,unit=None):\n",
    "    # Conversion factor from inches to centimeters\n",
    "    cm_per_inch = 2.54\n",
    "\n",
    "    if not pd.isna(dim_str):\n",
    "        dim_str = dim_str.strip()\n",
    "\n",
    "    if pd.isna(dim_str):\n",
    "        return dim_str\n",
    "\n",
    "\n",
    "    elif 'mm' in dim_str:\n",
    "        dimension_str_splitted = dim_str.split(' ')\n",
    "        return float(dimension_str_splitted[0]) / 10.0\n",
    "\n",
    "    elif 'in' in dim_str or unit=='inch':\n",
    "        # Convert inches to centimeters\n",
    "        cm = float(dim_str.split(' ')[0]) * cm_per_inch\n",
    "        return cm\n",
    "    elif 'cm' in dim_str:\n",
    "        dimension_str_splitted = dim_str.split(' ')\n",
    "        return dimension_str_splitted[0]\n",
    "\n",
    "    else:\n",
    "        processed_string = re.sub(r'[^\\d.]', '', dim_str)\n",
    "        return processed_string\n",
    "\n",
    "\n",
    "\n",
    "# Extracts values for depth, width, and height from text using flexible patterns\n",
    "def extract_dimensions_values(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    values = {'d': None, 'w': None, 'h': None}\n",
    "    matches = re.findall(r'(\\d+\\.\\d+|\\d+)[^\\d]*(d|w|h)', text)\n",
    "\n",
    "    for value, label in matches:\n",
    "        values[label] = value\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "# Conditionally assigns a sensor type based on the input text\n",
    "def assign_sensor_type(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if 'CMOS' in text:\n",
    "        return 'CMOS'\n",
    "    elif 'MOS' in text:\n",
    "        return 'MOS'\n",
    "    elif 'CCD' in text:\n",
    "        return 'CCD'\n",
    "    else:\n",
    "        return math.nan\n",
    "\n",
    "\n",
    "# Ensures the DataFrame has the specified columns, adding them if missing\n",
    "def ensure_columns_exist(df, column_list):\n",
    "    for col in column_list:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"/\"\n",
    "    return df\n",
    "\n",
    "\n",
    "# Deletes a file at the specified path\n",
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "# Removes empty CSV files in the specified directory\n",
    "def remove_empty_files(directory):\n",
    "    files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        if df.empty:\n",
    "            os.remove(file_path)\n",
    "\n",
    "# Counts the number of files with a given extension in a directory\n",
    "def count_files_in_directory(directory, extension='.csv'):\n",
    "    return len([file for file in os.listdir(directory) if file.endswith(extension)])\n",
    "\n",
    "# Counts the total number of records across all CSV files in a directory\n",
    "def count_total_records(directory):\n",
    "    total_records = 0\n",
    "    files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        total_records += df.shape[0]\n",
    "    return total_records\n",
    "\n",
    "# Deletes specified files or all files in a directory\n",
    "def delete_files(directory, files_to_delete='ALL'):\n",
    "    if files_to_delete == 'ALL':\n",
    "        files = os.listdir(directory)\n",
    "        if not files:\n",
    "            print(f\"No files to delete in {directory}\")\n",
    "            return\n",
    "        for file in files:\n",
    "            delete_file(os.path.join(directory, file))\n",
    "        print(f\"All files in {directory} have been deleted successfully.\")\n",
    "    else:\n",
    "        delete_file(os.path.join(directory, files_to_delete))\n",
    "\n",
    "# Calculates the Euclidean distance between a row and all rows in a DataFrame\n",
    "def calculate_row_distances(row, df, columns_to_consider):\n",
    "    distances = euclidean_distances([row[columns_to_consider]], df[columns_to_consider])\n",
    "    return distances.flatten()\n",
    "\n",
    "\n",
    "# Extracts the closest rows and calculates the match ratio for 'is_match' values\n",
    "def calculate_match_ratio(row, df, columns_to_consider):\n",
    "    distances = calculate_row_distances(row, df, columns_to_consider)\n",
    "    closest_indices = np.argsort(distances)[1:10]\n",
    "    closest_rows = df.iloc[closest_indices]\n",
    "    match_count = (closest_rows['is_match'] == row['is_match']).sum()\n",
    "    return match_count / len(closest_rows)\n",
    "\n",
    "\n",
    "# Checks if an element is present in any list within a list of lists\n",
    "def is_element_in_nested_list(nested_list, element):\n",
    "    for inner_list in nested_list:\n",
    "        if element in inner_list:\n",
    "            return True, inner_list\n",
    "    return False, []\n",
    "\n",
    "# Computes a weighted mean similarity score from a list of values\n",
    "def compute_weighted_mean_similarity(values_list):\n",
    "    weights = [30, 30, 10, 10, 2, 2, 2, 2, 2, 2]\n",
    "    values_transformed = [1 - x for x in values_list]\n",
    "    filtered_indices = [index for index, value in enumerate(values_transformed) if value == 3]\n",
    "\n",
    "    filtered_values = [value for index, value in enumerate(values_transformed) if index not in filtered_indices]\n",
    "    filtered_weights = [weight for index, weight in enumerate(weights) if index not in filtered_indices]\n",
    "\n",
    "    if np.sum(filtered_weights) > 0:\n",
    "        return np.average(filtered_values, weights=filtered_weights)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Prepares a DataFrame for similarity comparison by cleaning and filtering columns\n",
    "def prepare_dataframe_to_similarity_comparison(file_path):\n",
    "    \"\"\"\n",
    "    Prepares the DataFrame for similarity comparison by removing the 'is_match' column,\n",
    "    converting '/' to NaN, converting all columns to numeric, and filtering out columns\n",
    "    with a high percentage of NaN values.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The prepared DataFrame for similarity comparison.\n",
    "    \"\"\"\n",
    "    # Set a threshold for the percentage of NaN values\n",
    "    threshold_percentage = 70\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Check if 'is_match' column exists and remove it\n",
    "    if 'is_match' in df.columns:\n",
    "        df = df.drop(columns=['is_match'])\n",
    "\n",
    "    # Replace \"/\" with NaN and convert all columns to numeric\n",
    "    df.replace('/', 2, inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Filter columns where the percentage of NaN values is over the threshold\n",
    "    #filtered_columns = df.columns[df.isna().mean() < threshold_percentage / 100]\n",
    "    #df = df[filtered_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Prepares a DataFrame for prediction by cleaning and filtering columns\n",
    "def prepare_dataframe_prediction(df,relevant_columns):\n",
    "\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    df.replace('/', np.nan, inplace=True)\n",
    "\n",
    "    # Convert all columns to numerical data types\n",
    "    df.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Retrieves the number of rows in a similarity vector file\n",
    "def get_sim_vec_file_length(path_to_sim_vector_folder,file_name):\n",
    "    file_path = os.path.join(path_to_sim_vector_folder,file_name)\n",
    "    file_df = pd.read_csv(file_path)\n",
    "    return file_df.shape[0]\n",
    "\n",
    "\n",
    "# Creates a graph from record linkage tasks\n",
    "def create_graph(record_linkage_tasks):\n",
    "    # Create an empty undirected graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Extract unique values from the first and second columns\n",
    "    first_column_nodes = set(record_linkage_tasks['first_task'].unique())\n",
    "    second_column_nodes = set(record_linkage_tasks['second_task'].unique())\n",
    "\n",
    "    # Add nodes from the first column\n",
    "    G.add_nodes_from(first_column_nodes)\n",
    "\n",
    "    # Add nodes from the second column that are not in the first column\n",
    "    unique_second_column_nodes = second_column_nodes - first_column_nodes\n",
    "    G.add_nodes_from(unique_second_column_nodes)\n",
    "\n",
    "    # Add edges from the 'first_file' and 'second_file' columns\n",
    "    edges = record_linkage_tasks[['first_task', 'second_task']].values.tolist()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Relabel nodes to consecutive numbers starting from 1\n",
    "    mapping = {node: i + 1 for i, node in enumerate(G.nodes)}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    return G, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a18802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "# Suppress specific warning messages\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "\n",
    "\n",
    "def compute_mmd(X, Y, kernel='rbf', gamma=None):\n",
    "    \"\"\"\n",
    "    Compute the Maximum Mean Discrepancy (MMD) between two sets of samples.\n",
    "\n",
    "    Parameters:\n",
    "    X (ndarray): Array representing the first set of samples.\n",
    "    Y (ndarray): Array representing the second set of samples.\n",
    "    kernel (str or callable): Kernel function to use. Default is 'rbf' (Gaussian).\n",
    "    gamma (float): Parameter for the RBF kernel. If None, it is inferred from data.\n",
    "\n",
    "    Returns:\n",
    "    float: The MMD value.\n",
    "    \"\"\"\n",
    "    kernel = 'rbf' if kernel == 'squared_exp' else kernel\n",
    "    K_XX = pairwise_kernels(X, X, metric=kernel, gamma=gamma)\n",
    "    K_YY = pairwise_kernels(Y, Y, metric=kernel, gamma=gamma)\n",
    "    K_XY = pairwise_kernels(X, Y, metric=kernel, gamma=gamma)\n",
    "\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "\n",
    "    mmd_squared = (np.sum(K_XX) / (m * (m - 1)) -\n",
    "                   2 * np.sum(K_XY) / (m * n) +\n",
    "                   np.sum(K_YY) / (n * (n - 1)))\n",
    "\n",
    "    return np.sqrt(mmd_squared)\n",
    "\n",
    "\n",
    "def mmd_permutation_test(X, Y, num_permutations=100, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to assess the significance of MMD between two sets of samples.\n",
    "\n",
    "    Parameters:\n",
    "    X (ndarray): Array representing the first set of samples.\n",
    "    Y (ndarray): Array representing the second set of samples.\n",
    "    num_permutations (int): Number of permutations to perform.\n",
    "    **kwargs: Additional arguments to pass to the MMD function.\n",
    "\n",
    "    Returns:\n",
    "    float: The p-value of the permutation test.\n",
    "    \"\"\"\n",
    "    mmd_observed = compute_mmd(X, Y, **kwargs)\n",
    "    combined = np.vstack([X, Y])\n",
    "    n_samples1 = X.shape[0]\n",
    "\n",
    "    greater_extreme_count = 0\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        X_permuted, Y_permuted = combined[:n_samples1], combined[n_samples1:]\n",
    "        mmd_permuted = compute_mmd(X_permuted, Y_permuted, **kwargs)\n",
    "        if mmd_permuted >= mmd_observed:\n",
    "            greater_extreme_count += 1\n",
    "\n",
    "    return (greater_extreme_count + 1) / (num_permutations + 1)\n",
    "\n",
    "\n",
    "def calculate_psi(old_results, new_results):\n",
    "    \"\"\"\n",
    "    Computes the Population Stability Index (PSI) to measure the shift in distributions between two datasets.\n",
    "\n",
    "    Parameters:\n",
    "    old_results (array-like): Observed values from the original dataset.\n",
    "    new_results (array-like): Observed values from the new dataset.\n",
    "\n",
    "    Returns:\n",
    "    float: The PSI value indicating the shift in distributions.\n",
    "    \"\"\"\n",
    "    def psi(expected, actual):\n",
    "        return np.sum((actual - expected) * np.log(actual / expected))\n",
    "\n",
    "    old_expected = np.mean(old_results)\n",
    "    new_expected = np.mean(new_results)\n",
    "\n",
    "    return psi(old_expected, new_expected)\n",
    "\n",
    "\n",
    "def compare_linkage_tasks(task_1_path, task_2_path, task_1_name, task_2_name, test_type, relevant_columns, stat_lists=None, multivariate=False):\n",
    "    \"\"\"\n",
    "    Compares the distributions of two linkage tasks based on the specified test type.\n",
    "\n",
    "    Parameters:\n",
    "    task_1_path (str): Path to the first linkage task file.\n",
    "    task_2_path (str): Path to the second linkage task file.\n",
    "    task_1_name (str): Name of the first linkage task.\n",
    "    task_2_name (str): Name of the second linkage task.\n",
    "    test_type (str): The type of statistical test to perform ('ks_test', 'wasserstein_distance', 'calculate_psi', 'ML_based', or 'MMD').\n",
    "    relevant_columns (list): List of relevant columns to compare.\n",
    "    stat_lists (dict): Dictionary to store statistical test results for each column.\n",
    "    multivariate (bool): Flag indicating if the test is multivariate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Names of the compared files and a list of resulting values from the statistical tests.\n",
    "    \"\"\"\n",
    "    if multivariate:\n",
    "        if test_type == 'ML_based':\n",
    "            df_1 = prepare_dataframe_to_similarity_comparison(task_1_path)\n",
    "            df_2 = prepare_dataframe_to_similarity_comparison(task_2_path)\n",
    "            df_1['is_match'] = 0\n",
    "            df_2['is_match'] = 1\n",
    "\n",
    "            df_shuffled = pd.concat([df_1.sample(frac=1, random_state=42), df_2.sample(frac=1, random_state=42)], ignore_index=True)\n",
    "            X, y = df_shuffled.drop(columns=['is_match']), df_shuffled['is_match']\n",
    "\n",
    "            model = xgb.XGBClassifier(objective='binary:logistic')\n",
    "            cv_score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "            return task_1_name, task_2_name, cv_score\n",
    "\n",
    "        elif test_type == 'MMD':\n",
    "            df_1 = pd.read_csv(task_1_path)[relevant_columns].apply(pd.to_numeric, errors='coerce').fillna(2)\n",
    "            df_2 = pd.read_csv(task_2_path)[relevant_columns].apply(pd.to_numeric, errors='coerce').fillna(2)\n",
    "            mmd_value = mmd_permutation_test(df_1, df_2)\n",
    "            return task_1_name, task_2_name, mmd_value\n",
    "\n",
    "    else:\n",
    "        df_1 = prepare_dataframe_to_similarity_comparison(task_1_path)\n",
    "        df_2 = prepare_dataframe_to_similarity_comparison(task_2_path)\n",
    "        intersection_columns = df_1.columns.intersection(df_2.columns)\n",
    "        results = []\n",
    "\n",
    "        for column in stat_lists:\n",
    "            if column not in intersection_columns:\n",
    "                stat_lists[column].append(-2)\n",
    "            else:\n",
    "                if test_type == 'ks_test':\n",
    "                    ks_stat, p_value = ks_2samp(df_1[column], df_2[column])\n",
    "                    stat_lists[column].append(p_value)\n",
    "                    results.append(p_value)\n",
    "                elif test_type == 'wasserstein_distance':\n",
    "                    w_dist = wasserstein_distance(df_1[column], df_2[column])\n",
    "                    stat_lists[column].append(w_dist)\n",
    "                    results.append(w_dist)\n",
    "                elif test_type == 'calculate_psi':\n",
    "                    psi_value = calculate_psi(df_1[column], df_2[column])\n",
    "                    stat_lists[column].append(psi_value)\n",
    "                    results.append(psi_value)\n",
    "\n",
    "        return task_1_name, task_2_name, results\n",
    "\n",
    "\n",
    "def evaluate_similarity(results, test_type, case, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Evaluates the similarity between two files based on the test results.\n",
    "\n",
    "    Parameters:\n",
    "    results (list or float): Resulting value(s) from the statistical tests.\n",
    "    test_type (str): The type of statistical test performed.\n",
    "    case (int): Case number to determine the evaluation logic (1 or 2).\n",
    "    alpha (float): Significance level for the ks_test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    int: 1 if the files are similar, 0 otherwise.\n",
    "    \"\"\"\n",
    "    if test_type == 'ML_based':\n",
    "        return 0 if results > 0.80 else 1\n",
    "\n",
    "    elif test_type == 'MMD':\n",
    "        return 0 if results < 0.05 else 1\n",
    "\n",
    "    elif isinstance(results, list):\n",
    "        if case == 1:  # All features should have the same distribution\n",
    "            if test_type == 'ks_test':\n",
    "                return 0 if any(value < alpha for value in results) else 1\n",
    "            else:  # 'wasserstein_distance' or 'calculate_psi'\n",
    "                return 0 if any(value > 0.1 for value in results) else 1\n",
    "        else:  # Majority of features should have the same distribution\n",
    "            threshold = (lambda x: x > alpha) if test_type == 'ks_test' else (lambda x: x < 0.1)\n",
    "            similar_count = sum(threshold(value) for value in results)\n",
    "            return 1 if similar_count >= len(results) // 2 else 0\n",
    "\n",
    "\n",
    "def compute_similarity_test(case, test_type, tasks_path, relevant_columns, multivariate=False):\n",
    "    \"\"\"\n",
    "    Computes the similarity between pairs of record linkage tasks using various statistical tests.\n",
    "\n",
    "    Parameters:\n",
    "    case (int): Determines the logic for evaluating similarity (1 or 2).\n",
    "    test_type (str): The type of statistical test to perform.\n",
    "    tasks_path (str): Path to the folder containing the record linkage tasks.\n",
    "    relevant_columns (list): List of relevant columns to compare.\n",
    "    multivariate (bool): Flag indicating if the test is multivariate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A DataFrame containing similar record linkage tasks and a general DataFrame with all comparisons.\n",
    "    \"\"\"\n",
    "    stat_lists = {col: [] for col in relevant_columns} if not multivariate else None\n",
    "\n",
    "    first_tasks, second_tasks, similarities, processed_pairs = [], [], [], []\n",
    "    alpha = 0.05\n",
    "\n",
    "    task_files = [file for file in os.listdir(tasks_path) if file.endswith('.csv')]\n",
    "\n",
    "    for task_1 in task_files:\n",
    "        task_1_path = os.path.join(tasks_path, task_1)\n",
    "        for task_2 in task_files:\n",
    "            task_2_path = os.path.join(tasks_path, task_2)\n",
    "            pair_identifier = f\"{task_1}_{task_2}\"\n",
    "            reverse_pair = f\"{task_2}_{task_1}\"\n",
    "\n",
    "            if task_1_path != task_2_path and pair_identifier not in processed_pairs and reverse_pair not in processed_pairs:\n",
    "                processed_pairs.append(pair_identifier)\n",
    "\n",
    "                file1, file2, results = compare_linkage_tasks(\n",
    "                    task_1_path, task_2_path, task_1, task_2, test_type, relevant_columns, stat_lists, multivariate\n",
    "                )\n",
    "                similarity = evaluate_similarity(results, test_type, case, alpha)\n",
    "\n",
    "                first_tasks.append(file1)\n",
    "                second_tasks.append(file2)\n",
    "                similarities.append(similarity)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'first_task': first_tasks,\n",
    "        'second_task': second_tasks,\n",
    "        **stat_lists,\n",
    "        'similarity': similarities\n",
    "    }) if not multivariate else pd.DataFrame({\n",
    "        'first_task': first_tasks,\n",
    "        'second_task': second_tasks,\n",
    "        'similarity': similarities\n",
    "    })\n",
    "\n",
    "    similar_tasks_df = results_df[results_df['similarity'] == 1]\n",
    "\n",
    "    if not multivariate:\n",
    "        results_df['avg_similarity'] = results_df.apply(lambda row: compute_weighted_mean_similarity(row[2:-1]), axis=1)\n",
    "        similar_tasks_df['avg_similarity'] = similar_tasks_df.apply(lambda row: compute_weighted_mean_similarity(row[2:-1]), axis=1)\n",
    "\n",
    "    # Output statistics\n",
    "    num_similar_tasks = similar_tasks_df.shape[0]\n",
    "    print(f\"Number of tasks with similar distribution: {num_similar_tasks}\")\n",
    "\n",
    "    unique_first_tasks = similar_tasks_df['first_task'].unique()\n",
    "    unique_second_tasks = similar_tasks_df['second_task'].unique()\n",
    "\n",
    "    unique_tasks_count = len(set(unique_first_tasks) | set(unique_second_tasks))\n",
    "    print(f\"Total number of unique tasks with similar distribution: {unique_tasks_count}\")\n",
    "\n",
    "    return similar_tasks_df, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95022f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "from networkx.algorithms.community import asyn_lpa_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import sys\n",
    "\n",
    "\n",
    "def detect_communities_using_girvan_newman(graph):\n",
    "    \"\"\"\n",
    "    Detect communities in the given graph using the Girvan-Newman algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The graph on which to perform community detection.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of communities, each community being a list of nodes.\n",
    "    \"\"\"\n",
    "    # Apply the Girvan-Newman algorithm to find communities\n",
    "    community_generator = girvan_newman(graph)\n",
    "\n",
    "    # Get the first set of communities\n",
    "    first_communities = next(community_generator)\n",
    "    communities = [list(community) for community in first_communities]\n",
    "\n",
    "    # Calculate and print the modularity\n",
    "    modularity_value = modularity(graph, first_communities)\n",
    "\n",
    "    # Print the number of communities and the modularity value\n",
    "    print(f\"Number of communities: {len(communities)}\")\n",
    "    print(f\"Modularity: {modularity_value}\")\n",
    "\n",
    "    return communities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_communities_using_label_propagation_clustering(graph):\n",
    "    \"\"\"\n",
    "    Detect communities in the given graph using the Asynchronous Label Propagation algorithm\n",
    "    and print the communities and modularity value.\n",
    "\n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The graph on which to perform community detection.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of communities, each community being a list of nodes.\n",
    "    \"\"\"\n",
    "    # Detect communities using asynchronous label propagation\n",
    "    communities = list(asyn_lpa_communities(graph))\n",
    "\n",
    "    # Calculate and print modularity\n",
    "    modularity_value = modularity(graph, communities)\n",
    "\n",
    "    # Print the number of communities and the modularity value\n",
    "    print(f\"Number of communities: {len(communities)}\")\n",
    "    print(f\"Modularity: {modularity_value}\")\n",
    "\n",
    "    return communities\n",
    "\n",
    "\n",
    "def detect_communities(community_detection_algorithum,graph):\n",
    "\n",
    "    if community_detection_algorithum == 'girvan_newman':\n",
    "       communities = detect_communities_using_girvan_newman(graph)\n",
    "       return communities\n",
    "\n",
    "    elif community_detection_algorithum == 'label_propagation_clustering':\n",
    "       communities = detect_communities_using_label_propagation_clustering(graph)\n",
    "       return communities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c06b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/Users/abdulnaser/Desktop/Masterarbeit/metadatatransferlearning-main/meta_tl/')\n",
    "from utils import *\n",
    "\n",
    "def select_largest_file_in_community(path_to_sim_vector_folder,community, node_mapping):\n",
    "    \"\"\"\n",
    "    Select the largest file in each community and return the largest file and a list of other linkage tasks.\n",
    "\n",
    "    Parameters:\n",
    "    community (list): A list of nodes representing the community.\n",
    "    node_mapping (dict): A dictionary mapping node labels to community nodes.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the largest file and a list of other linkage tasks.\n",
    "    \"\"\"\n",
    "    # Identify linkage tasks that belong to the community\n",
    "    community_tasks = [task for task, node in node_mapping.items() if node in community]\n",
    "\n",
    "    # Find the task with the maximum file count\n",
    "    largest_task = None\n",
    "    largest_task_count = -1\n",
    "    for task in community_tasks:\n",
    "        task_count = get_sim_vec_file_length(path_to_sim_vector_folder,task)\n",
    "        if task_count > largest_task_count:\n",
    "            largest_task = task\n",
    "            largest_task_count = task_count\n",
    "\n",
    "    # Create a list of other tasks in the community\n",
    "    other_tasks = [task for task in community_tasks if task != largest_task]\n",
    "\n",
    "    return largest_task, other_tasks\n",
    "\n",
    "\n",
    "def select_linkage_tasks_from_communities(path_to_sim_vector_folder,linkage_tasks_communities, node_mapping):\n",
    "    \"\"\"\n",
    "    Loop over each community to select the largest file and related linkage tasks,\n",
    "    then store the results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    linkage_tasks_communities (list): A list of communities, each community being a list of nodes.\n",
    "    node_mapping (dict): A dictionary mapping node labels to community nodes.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the largest file and values are lists of other linkage tasks.\n",
    "    \"\"\"\n",
    "    selected_tasks_dict = {}\n",
    "\n",
    "    for community in linkage_tasks_communities:\n",
    "        largest_task, other_tasks = select_largest_file_in_community(path_to_sim_vector_folder,community, node_mapping)\n",
    "        selected_tasks_dict[largest_task] = other_tasks\n",
    "\n",
    "    return selected_tasks_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8456fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "def allocate_active_learning_budget(selected_tasks, tasks_dir, tasks_info_df, min_budget_per_task, total_budget):\n",
    "    \"\"\"\n",
    "    Allocate the active learning budget to both community (non-singleton) and singleton tasks.\n",
    "\n",
    "    Parameters:\n",
    "    selected_tasks (dict): A dictionary of selected tasks.\n",
    "    tasks_dir (str): The directory containing the linkage tasks files.\n",
    "    tasks_info_df (pd.DataFrame): DataFrame containing information about linkage tasks.\n",
    "    min_budget_per_task (int): The minimum budget to allocate per task.\n",
    "    total_budget (int): The total budget available for allocation.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with task names as keys and their allocated budgets as values.\n",
    "    \"\"\"\n",
    "    budget_allocation = {}\n",
    "\n",
    "    # Allocate budget for community (non-singleton) tasks\n",
    "    for task in selected_tasks:\n",
    "        task_path = os.path.join(tasks_dir, task)\n",
    "        task_df = pd.read_csv(task_path)\n",
    "        budget_allocation[task] = task_df.shape[0]\n",
    "\n",
    "    # Identify singleton tasks (tasks that do not belong to any community)\n",
    "    non_singleton_tasks = tasks_info_df[tasks_info_df['similarity'] == 1]['first_task'].tolist()\n",
    "    all_tasks = set(tasks_info_df['first_task'].unique()).union(tasks_info_df['second_task'].unique())\n",
    "\n",
    "    # Allocate budget for singleton tasks\n",
    "    for task in all_tasks:\n",
    "        if task not in non_singleton_tasks:\n",
    "            task_path = os.path.join(tasks_dir, task)\n",
    "            task_df = pd.read_csv(task_path)\n",
    "            budget_allocation[task] = task_df.shape[0]\n",
    "\n",
    "    # Calculate the total minimum budget required\n",
    "    total_min_budget = min_budget_per_task * len(budget_allocation)\n",
    "\n",
    "    # Ensure the total minimum budget does not exceed the total available budget\n",
    "    if total_min_budget > total_budget:\n",
    "        raise ValueError(\"The total minimum budget exceeds the total available budget.\")\n",
    "\n",
    "    # Calculate the remaining budget after allocating the minimum budget\n",
    "    remaining_budget = total_budget - total_min_budget\n",
    "\n",
    "    # Calculate the proportional allocations for the remaining budget\n",
    "    total_size = sum(budget_allocation.values())\n",
    "    proportional_allocations = {\n",
    "        task: (size / total_size) * remaining_budget for task, size in budget_allocation.items()\n",
    "    }\n",
    "\n",
    "    # Combine the minimum budget with the proportional allocations and round up the final allocations\n",
    "    final_allocations = {\n",
    "        task: math.ceil(min_budget_per_task + budget) for task, budget in proportional_allocations.items()\n",
    "    }\n",
    "\n",
    "    return final_allocations\n",
    "\n",
    "\n",
    "def active_learning_bootstrap(task_df, iteration_budget, total_budget, num_classifiers=5):\n",
    "    \"\"\"\n",
    "    Apply active learning using bootstrapping to label the records in the linkage task.\n",
    "\n",
    "    Parameters:\n",
    "    task_df (pd.DataFrame): DataFrame containing the task data.\n",
    "    iteration_budget (int): The budget for each iteration.\n",
    "    total_budget (int): The total budget available.\n",
    "    num_classifiers (int): The number of classifiers to use for bootstrapping.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The labeled DataFrame.\n",
    "    int: The total budget used.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initial training data selection using a random sample\n",
    "    seed_indices = np.random.choice(task_df.shape[0], iteration_budget, replace=False)\n",
    "    training_df = task_df.iloc[seed_indices]\n",
    "\n",
    "    # Ensure diverse initial training set by checking for label diversity\n",
    "    while len(training_df['is_match'].unique()) <= 1:\n",
    "        seed_indices = np.random.choice(task_df.shape[0], iteration_budget, replace=False)\n",
    "        training_df = task_df.iloc[seed_indices]\n",
    "\n",
    "    # Create the unlabeled dataset by removing the selected training data\n",
    "    unlabeled_df = task_df.drop(seed_indices)\n",
    "    used_budget = training_df.shape[0]\n",
    "\n",
    "    # Continue the active learning process until the budget is exhausted\n",
    "    while used_budget < total_budget:\n",
    "        classifiers = []\n",
    "\n",
    "        # Train multiple classifiers using bootstrapping\n",
    "        for _ in range(num_classifiers):\n",
    "            clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "            bootstrap_indices = np.random.choice(training_df.shape[0], len(training_df), replace=True)\n",
    "            bootstrap_df = training_df.iloc[bootstrap_indices]\n",
    "\n",
    "            # Ensure label diversity in the bootstrapped dataset\n",
    "            while len(bootstrap_df['is_match'].unique()) <= 1:\n",
    "                bootstrap_indices = np.random.choice(training_df.shape[0], len(training_df), replace=True)\n",
    "                bootstrap_df = training_df.iloc[bootstrap_indices]\n",
    "\n",
    "            # Train the classifier on the bootstrapped dataset\n",
    "            X_train = bootstrap_df.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')\n",
    "            y_train = bootstrap_df['is_match']\n",
    "            clf.fit(X_train, y_train)\n",
    "            classifiers.append(clf)\n",
    "\n",
    "        # Calculate uncertainty for each unlabeled example using the ensemble of classifiers\n",
    "        uncertainties = {\n",
    "            idx: np.mean([clf.predict([record.iloc[2:-1]])[0] for clf in classifiers]) * (1 - np.mean([clf.predict([record.iloc[2:-1]])[0] for clf in classifiers]))\n",
    "            for idx, record in unlabeled_df.iterrows()\n",
    "        }\n",
    "\n",
    "        # Select the next batch of examples based on uncertainty\n",
    "        current_iteration_budget = min(iteration_budget, total_budget - used_budget)\n",
    "        selected_indices = sorted(uncertainties.items(), key=operator.itemgetter(1), reverse=True)[:current_iteration_budget]\n",
    "        next_batch_indices = [idx for idx, _ in selected_indices]\n",
    "\n",
    "        # Add the selected examples to the training set and remove them from the unlabeled set\n",
    "        new_training_df = unlabeled_df.loc[next_batch_indices]\n",
    "        unlabeled_df = unlabeled_df.drop(next_batch_indices)\n",
    "        training_df = pd.concat([training_df, new_training_df])\n",
    "\n",
    "        # Update the used budget\n",
    "        used_budget = training_df.shape[0]\n",
    "        print(f\"Elapsed time: {time.time() - start_time:.2f} seconds, Used budget: {used_budget}\")\n",
    "\n",
    "    # Train the final classifier on the full training set\n",
    "    final_clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "    X_train = training_df.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')\n",
    "    y_train = training_df['is_match']\n",
    "    final_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the entire task dataset\n",
    "    X_test = task_df.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')\n",
    "    task_df['pred'] = final_clf.predict(X_test)\n",
    "\n",
    "    return task_df, used_budget\n",
    "\n",
    "\n",
    "def active_learning_margin(task_df, iteration_budget, total_budget):\n",
    "    \"\"\"\n",
    "    Apply active learning using margin sampling to label the records in the linkage task.\n",
    "\n",
    "    Parameters:\n",
    "    task_df (pd.DataFrame): DataFrame containing the task data.\n",
    "    iteration_budget (int): The budget for each iteration.\n",
    "    total_budget (int): The total budget available.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The labeled DataFrame.\n",
    "    int: The total budget used.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initial training data selection using a random sample\n",
    "    seed_indices = np.random.choice(task_df.shape[0], iteration_budget, replace=False)\n",
    "    training_df = task_df.iloc[seed_indices]\n",
    "\n",
    "    # Ensure diverse initial training set by checking for label diversity\n",
    "    while len(training_df['is_match'].unique()) <= 1:\n",
    "        seed_indices = np.random.choice(task_df.shape[0], iteration_budget, replace=False)\n",
    "        training_df = task_df.iloc[seed_indices]\n",
    "\n",
    "    # Create the unlabeled dataset by removing the selected training data\n",
    "    unlabeled_df = task_df.drop(seed_indices)\n",
    "    used_budget = training_df.shape[0]\n",
    "\n",
    "    # Continue the active learning process until the budget is exhausted\n",
    "    while used_budget < total_budget:\n",
    "        clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "        X_train = training_df.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')\n",
    "        y_train = training_df['is_match']\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate margin for each unlabeled example\n",
    "        margins = {\n",
    "            idx: np.abs(clf.predict_proba([record.iloc[2:-1]])[0][0] - clf.predict_proba([record.iloc[2:-1]])[0][1])\n",
    "            for idx, record in unlabeled_df.iterrows()\n",
    "        }\n",
    "\n",
    "        # Select the next batch of examples based on smallest margins\n",
    "        current_iteration_budget = min(iteration_budget, total_budget - used_budget)\n",
    "        selected_indices = sorted(margins.items(), key=lambda x: x[1])[:current_iteration_budget]\n",
    "        next_batch_indices = [idx for idx, _ in selected_indices]\n",
    "\n",
    "        # Add the selected examples to the training set and remove them from the unlabeled set\n",
    "        new_training_df = unlabeled_df.loc[next_batch_indices]\n",
    "        unlabeled_df = unlabeled_df.drop(next_batch_indices)\n",
    "        training_df = pd.concat([training_df, new_training_df])\n",
    "\n",
    "        # Update the used budget\n",
    "        used_budget = training_df.shape[0]\n",
    "        print(f\"Elapsed time: {time.time() - start_time:.2f} seconds, Used budget: {used_budget}\")\n",
    "\n",
    "    # Train the final classifier on the full training set\n",
    "    final_clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "    X_train = training_df.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')\n",
    "    y_train = training_df['is_match']\n",
    "    final_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the entire task dataset\n",
    "    X_test = task_df.iloc[:, 2:-1].apply(pd.to_numeric, errors='coerce')\n",
    "    task_df['pred'] = final_clf.predict(X_test)\n",
    "\n",
    "    return task_df, used_budget\n",
    "\n",
    "\n",
    "def label_linkage_tasks(selected_tasks, tasks_dir, tasks_info_df, min_budget, total_budget, labeled_tasks_dir, active_learning_strategy, relevant_columns):\n",
    "    \"\"\"\n",
    "    Allocate budgets to selected linkage tasks and apply active learning to label them.\n",
    "\n",
    "    Parameters:\n",
    "    selected_tasks (dict): A dictionary of selected tasks.\n",
    "    tasks_dir (str): The directory containing the linkage tasks files.\n",
    "    tasks_info_df (pd.DataFrame): DataFrame containing information about linkage tasks.\n",
    "    min_budget (int): The minimum budget to allocate per task.\n",
    "    total_budget (int): The total available budget for allocation.\n",
    "    labeled_tasks_dir (str): The directory where the labeled tasks will be saved.\n",
    "    active_learning_strategy (str): The active learning strategy to use (\"bootstrapping\", \"margin\").\n",
    "    relevant_columns (list): List of columns to be used for active learning.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Clear the labeled tasks directory\n",
    "    delete_files(labeled_tasks_dir)\n",
    "\n",
    "    # Allocate budgets to linkage tasks\n",
    "    allocated_budgets = allocate_active_learning_budget(selected_tasks, tasks_dir, tasks_info_df, min_budget, total_budget)\n",
    "\n",
    "    # Map active learning strategies to functions\n",
    "    strategy_map = {\n",
    "        \"bootstrapping\": active_learning_bootstrap,\n",
    "        \"margin\": active_learning_margin,\n",
    "    }\n",
    "\n",
    "    # Validate the active learning strategy\n",
    "    if active_learning_strategy not in strategy_map:\n",
    "        raise ValueError(f\"Invalid active learning strategy: {active_learning_strategy}\")\n",
    "\n",
    "    # Process each linkage task\n",
    "    for task, budget in allocated_budgets.items():\n",
    "        task_path = os.path.join(tasks_dir, task)\n",
    "        task_df = pd.read_csv(task_path)\n",
    "\n",
    "        # Prepare the dataframe for prediction\n",
    "        processed_df = prepare_dataframe_prediction(task_df, relevant_columns)\n",
    "        print(f\"Processing task: {task}, initial shape: {processed_df.shape[0]}\")\n",
    "\n",
    "        # Apply the selected active learning strategy\n",
    "        active_learning_function = strategy_map[active_learning_strategy]\n",
    "        if task != 'www.canon-europe.com_cammarkt.com.csv':\n",
    "            labeled_df, used_budget = active_learning_function(processed_df, min_budget, budget)\n",
    "            print(f\"Task {task} labeled with a budget of {used_budget}\")\n",
    "\n",
    "            # Save the labeled task\n",
    "            labeled_df.to_csv(os.path.join(labeled_tasks_dir, task), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43b7041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfear_learning_process_main(selected_linkage_tasks_from_communities,active_learning_folder,path_to_record_linkage_tasks,relevant_columns):\n",
    "    for selected_linkage_task, coressponding_linkage_tasks in selected_linkage_tasks_from_communities.items():\n",
    "        if selected_linkage_task != 'www.wexphotographic.com_www.price-hunt.com.csv':# because after labeling it is all 1\n",
    "            print(f\"Train the model on the selected labeled task which is: {selected_linkage_task}\")\n",
    "            selected_linkage_task_df_processed = pd.read_csv(active_learning_folder + selected_linkage_task)\n",
    "            #selected_linkage_task_df_processed = prepare_dataframe_prediction(selected_linkage_task_df,relevant_columns)\n",
    "\n",
    "            X = selected_linkage_task_df_processed.iloc[:, 2:-2]\n",
    "            y = selected_linkage_task_df_processed.iloc[: , -1]\n",
    "\n",
    "            model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            print(f\"Inference on the coresponding tasks!\")\n",
    "            for coressponding_linkage_task in coressponding_linkage_tasks:\n",
    "                coressponding_linkage_task_df = pd.read_csv(path_to_record_linkage_tasks + coressponding_linkage_task)\n",
    "                coressponding_linkage_task_df_processed = prepare_dataframe_prediction(coressponding_linkage_task_df,relevant_columns)\n",
    "\n",
    "                X = coressponding_linkage_task_df_processed.iloc[:, 2:-1] # Features (all columns except the last one)\n",
    "                y = coressponding_linkage_task_df_processed.iloc[: , -1] # Taregt variable (is_match)\n",
    "\n",
    "                # Prediction\n",
    "                predictions = model.predict(X)\n",
    "                class_probs = model.predict_proba(X)\n",
    "                coressponding_linkage_task_df_processed['pred'] = predictions\n",
    "                coressponding_linkage_task_df_processed[['probabilties_0', 'probabilties_1']] = class_probs\n",
    "                coressponding_linkage_task_df_processed.to_csv(os.path.join(active_learning_folder,coressponding_linkage_task))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9faac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "def evaluate_predictions(predictions_folder):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of active learning predictions by calculating F1 score, precision, and recall.\n",
    "\n",
    "    Parameters:\n",
    "    predictions_folder (str): The path to the folder containing the prediction CSV files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # List to store individual DataFrames from all prediction files\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate through each file in the predictions folder\n",
    "    for filename in os.listdir(predictions_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct the full path to the file\n",
    "            filepath = os.path.join(predictions_folder, filename)\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            # Modify the record comparison columns to include the file identifiers\n",
    "            file_identifiers = filename.split('_')\n",
    "            df['record_compared_1'] = df['record_compared_1'] + \"_\" + file_identifiers[0]\n",
    "            df['record_compared_2'] = df['record_compared_2'] + \"_\" + file_identifiers[1]\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Fill missing predictions with the actual labels\n",
    "    combined_df.loc[combined_df['pred'].isna(), 'pred'] = combined_df.loc[combined_df['pred'].isna(), 'is_match']\n",
    "\n",
    "    # Output the size of the combined DataFrame\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape[0]}\")\n",
    "\n",
    "    # Calculate and print the evaluation metrics\n",
    "    f1 = f1_score(combined_df['is_match'], combined_df['pred'])\n",
    "    precision = precision_score(combined_df['is_match'], combined_df['pred'])\n",
    "    recall = recall_score(combined_df['is_match'], combined_df['pred'])\n",
    "\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Example usage (assuming the predictions folder path is defined):\n",
    "# evaluate_predictions('/path/to/predictions/folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc079f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-empty record linkage tasks: 146\n",
      "Number of unique cleaned data sources: 22\n",
      "Total number of record pairs: 378877\n",
      "Number of tasks with similar distribution: 1155\n",
      "Total number of unique tasks with similar distribution: 114\n",
      "Elapsed time for similarity test: 316.47 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define paths to various data directories\n",
    "RECORD_LINKAGE_TASKS_PATH = os.path.join(MAIN_PATH, 'data/linkage_tasks/')\n",
    "CLEANED_DATA_SOURCES_PATH = os.path.join(MAIN_PATH, 'data/cleaned_data/')\n",
    "LABELED_RECORD_LINKAGE_TASKS_PATH = os.path.join(MAIN_PATH, 'data/linkage_tasks_labeled/')\n",
    "\n",
    "\n",
    "# Define the configuration parameters\n",
    "STATISTICAL_TEST = 'calculate_psi' # or\n",
    "FEATURE_CASE = 1  # 1 for all features to have the same distribution, 2 for majority of features to have the same distributions\n",
    "COMMUNITY_DETECTION_ALGORITHM = 'girvan_newman'  # or 'label_propagation_clustering'\n",
    "ACTIVE_LEARNING_ALGORITHM = 'bootstrapping'  # or 'margin'\n",
    "ACTIVE_LEARNING_ITERATION_BUDGET = 20\n",
    "ACTIVE_LEARNING_TOTAL_BUDGET = 1000\n",
    "\n",
    "RELEVANT_COLUMNS_IN_LINKAGE_TASKS = ['Produktname_dic3', 'Modell_Liste_3g', 'MPN_Liste_TruncateBegin20',\n",
    "            'EAN_Liste_TruncateBegin20', 'Digital_zoom_NumMaxProz30',\n",
    "            'optischer_zoom_NumMaxProz30', 'Breite_NumMaxProz30', 'HÃ¶he_NumMaxProz30',\n",
    "            'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3']\n",
    "\n",
    "RELEVANT_COLUMNS_IN_ACTIVE_LEARNING = ['record_compared_1','record_compared_2','MPN_Liste_TruncateBegin20','EAN_Liste_TruncateBegin20','Produktname_dic3',\n",
    "               'Modell_Liste_3g','Digital_zoom_NumMaxProz30','optischer_zoom_NumMaxProz30',\n",
    "               'Breite_NumMaxProz30', 'HÃ¶he_NumMaxProz30', 'Gewicht_NumMaxProz30', 'Sensortyp_Jaccard3','is_match']\n",
    "\n",
    "# ===================================================\n",
    "# Step 1: Prepare Record Linkage Tasks\n",
    "# ===================================================\n",
    "\n",
    "# Remove empty record linkage tasks\n",
    "remove_empty_files(RECORD_LINKAGE_TASKS_PATH)\n",
    "\n",
    "# Count and print the number of non-empty record linkage tasks\n",
    "num_linkage_tasks = count_files_in_directory(RECORD_LINKAGE_TASKS_PATH)\n",
    "print(f\"Number of non-empty record linkage tasks: {num_linkage_tasks}\")\n",
    "\n",
    "# Count and print the number of unique data sources in the cleaned data\n",
    "num_cleaned_data_sources = count_files_in_directory(CLEANED_DATA_SOURCES_PATH)\n",
    "print(f\"Number of unique cleaned data sources: {num_cleaned_data_sources}\")\n",
    "\n",
    "# Count and print the total number of record pairs across all tasks\n",
    "total_record_pairs = count_total_records(RECORD_LINKAGE_TASKS_PATH)\n",
    "print(f\"Total number of record pairs: {total_record_pairs}\")\n",
    "\n",
    "# ===================================================\n",
    "# Step 2: Perform Linkage Tasks Distribution Test\n",
    "# ===================================================\n",
    "\n",
    "# Record the start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute the similarity test based on the chosen statistical test and case\n",
    "linkage_tasks_similarity_df, linkage_tasks_general_df = compute_similarity_test(\n",
    "    FEATURE_CASE, STATISTICAL_TEST, RECORD_LINKAGE_TASKS_PATH,RELEVANT_COLUMNS_IN_LINKAGE_TASKS\n",
    ")\n",
    "\n",
    "# Calculate and print the elapsed time for the similarity test\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time for similarity test: {elapsed_time:.2f} seconds\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2852fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities: 6\n",
      "Modularity: 0.027352935664624007\n",
      "Elapsed time for graph clustering: 0.48 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a graph of linkage tasks based on their similarity\n",
    "graph, task_mapping = create_graph(linkage_tasks_similarity_df)\n",
    "\n",
    "# Record the start time for graph clustering\n",
    "start_time = time.time()\n",
    "\n",
    "# Detect communities within the graph using the selected algorithm\n",
    "linkage_task_communities = detect_communities(\n",
    "    COMMUNITY_DETECTION_ALGORITHM, graph\n",
    ")\n",
    "\n",
    "# Calculate and print the elapsed time for graph clustering\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time for graph clustering: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df434bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for task selection: 0.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record the start time for task selection\n",
    "start_time = time.time()\n",
    "\n",
    "# Select the largest task from each community for model training\n",
    "selected_tasks = select_linkage_tasks_from_communities(\n",
    "    RECORD_LINKAGE_TASKS_PATH, linkage_task_communities, task_mapping\n",
    ")\n",
    "\n",
    "# Calculate and print the elapsed time for task selection\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time for task selection: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "debd5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ebay.com_www.priceme.co.nz.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_www.mypriceindia.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.wexphotographic.com_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.eglobalcentral.co.uk.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/buy.net_www.canon-europe.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ilgs.net_www.gosale.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.walmart.com_www.pricedekho.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.mypriceindia.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.wexphotographic.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.pcconnection.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.walmart.com_www.shopmania.in.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ebay.com_www.gosale.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.wexphotographic.com_www.mypriceindia.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.gosale.com_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_www.wexphotographic.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_www.ebay.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.wexphotographic.com_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.wexphotographic.com_www.henrys.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.cambuy.com.au_www.shopmania.in.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.henrys.com_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_www.canon-europe.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ilgs.net_www.wexphotographic.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_www.henrys.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.walmart.com_www.cambuy.com.au.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.price-hunt.com_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ebay.com_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ilgs.net_www.ebay.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.henrys.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.shopmania.in_www.pricedekho.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ilgs.net_cammarkt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ukdigitalcameras.co.uk_www.shopmania.in.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.henrys.com_www.pcconnection.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.cambuy.com.au_www.pricedekho.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.henrys.com_www.mypriceindia.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ilgs.net_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.mypriceindia.com_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.ilgs.net.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.buzzillions.com_www.gosale.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.ebay.com_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.cambuy.com.au_www.ukdigitalcameras.co.uk.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.wexphotographic.com_www.eglobalcentral.co.uk.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/buy.net_www.wexphotographic.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.mypriceindia.com_www.pcconnection.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.canon-europe.com_www.gosale.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/buy.net_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.henrys.com_www.price-hunt.com.csv deleted successfully.\n",
      "File /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/www.camerafarm.com.au_www.priceme.co.nz.csv deleted successfully.\n",
      "All files in /Users/abdulnaser/Desktop/TL_Multi_Source_ER/data/linkage_tasks_labeled/ have been deleted successfully.\n",
      "Processing task: www.ebay.com_www.priceme.co.nz.csv, initial shape: 33296\n",
      "Task www.ebay.com_www.priceme.co.nz.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_www.mypriceindia.com.csv, initial shape: 601\n",
      "Task www.buzzillions.com_www.mypriceindia.com.csv labeled with a budget of 20\n",
      "Processing task: www.ukdigitalcameras.co.uk_www.shopmania.in.csv, initial shape: 11627\n",
      "Task www.ukdigitalcameras.co.uk_www.shopmania.in.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_www.ebay.com.csv, initial shape: 22306\n",
      "Task www.buzzillions.com_www.ebay.com.csv labeled with a budget of 20\n",
      "Processing task: www.ebay.com_www.gosale.com.csv, initial shape: 39573\n",
      "Task www.ebay.com_www.gosale.com.csv labeled with a budget of 20\n",
      "Processing task: www.wexphotographic.com_www.price-hunt.com.csv, initial shape: 137\n",
      "Task www.wexphotographic.com_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_www.henrys.com.csv, initial shape: 234\n",
      "Task www.buzzillions.com_www.henrys.com.csv labeled with a budget of 20\n",
      "Processing task: www.henrys.com_cammarkt.com.csv, initial shape: 45\n",
      "Task www.henrys.com_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.henrys.com_www.pcconnection.com.csv, initial shape: 180\n",
      "Task www.henrys.com_www.pcconnection.com.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.eglobalcentral.co.uk.csv, initial shape: 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task www.canon-europe.com_www.eglobalcentral.co.uk.csv labeled with a budget of 20\n",
      "Processing task: www.cambuy.com.au_www.shopmania.in.csv, initial shape: 12927\n",
      "Task www.cambuy.com.au_www.shopmania.in.csv labeled with a budget of 20\n",
      "Processing task: www.wexphotographic.com_www.mypriceindia.com.csv, initial shape: 195\n",
      "Task www.wexphotographic.com_www.mypriceindia.com.csv labeled with a budget of 20\n",
      "Processing task: www.ilgs.net_cammarkt.com.csv, initial shape: 77\n",
      "Task www.ilgs.net_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_cammarkt.com.csv, initial shape: 467\n",
      "Task www.buzzillions.com_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.cambuy.com.au_www.pricedekho.com.csv, initial shape: 8203\n",
      "Task www.cambuy.com.au_www.pricedekho.com.csv labeled with a budget of 20\n",
      "Processing task: www.ilgs.net_www.gosale.com.csv, initial shape: 294\n",
      "Task www.ilgs.net_www.gosale.com.csv labeled with a budget of 20\n",
      "Processing task: www.gosale.com_cammarkt.com.csv, initial shape: 753\n",
      "Task www.gosale.com_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.ilgs.net_www.price-hunt.com.csv, initial shape: 98\n",
      "Task www.ilgs.net_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: buy.net_www.wexphotographic.com.csv, initial shape: 243\n",
      "Task buy.net_www.wexphotographic.com.csv labeled with a budget of 20\n",
      "Processing task: www.mypriceindia.com_www.pcconnection.com.csv, initial shape: 365\n",
      "Task www.mypriceindia.com_www.pcconnection.com.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.henrys.com.csv, initial shape: 37\n",
      "Task www.canon-europe.com_www.henrys.com.csv labeled with a budget of 20\n",
      "Processing task: www.henrys.com_www.price-hunt.com.csv, initial shape: 154\n",
      "Task www.henrys.com_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_www.canon-europe.com.csv, initial shape: 207\n",
      "Task www.buzzillions.com_www.canon-europe.com.csv labeled with a budget of 20\n",
      "Processing task: www.camerafarm.com.au_www.priceme.co.nz.csv, initial shape: 432\n",
      "Task www.camerafarm.com.au_www.priceme.co.nz.csv labeled with a budget of 20\n",
      "Processing task: www.henrys.com_www.mypriceindia.com.csv, initial shape: 238\n",
      "Task www.henrys.com_www.mypriceindia.com.csv labeled with a budget of 20\n",
      "Processing task: buy.net_www.price-hunt.com.csv, initial shape: 269\n",
      "Task buy.net_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_www.wexphotographic.com.csv, initial shape: 164\n",
      "Task www.buzzillions.com_www.wexphotographic.com.csv labeled with a budget of 20\n",
      "Processing task: www.ilgs.net_www.ebay.com.csv, initial shape: 3931\n",
      "Task www.ilgs.net_www.ebay.com.csv labeled with a budget of 20\n",
      "Processing task: www.ebay.com_www.price-hunt.com.csv, initial shape: 7796\n",
      "Task www.ebay.com_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: www.cambuy.com.au_www.ukdigitalcameras.co.uk.csv, initial shape: 2921\n",
      "Task www.cambuy.com.au_www.ukdigitalcameras.co.uk.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.price-hunt.com.csv, initial shape: 63\n",
      "Task www.canon-europe.com_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_cammarkt.com.csv, initial shape: 117\n",
      "Processing task: www.canon-europe.com_www.gosale.com.csv, initial shape: 353\n",
      "Task www.canon-europe.com_www.gosale.com.csv labeled with a budget of 20\n",
      "Processing task: www.shopmania.in_www.pricedekho.com.csv, initial shape: 33891\n",
      "Task www.shopmania.in_www.pricedekho.com.csv labeled with a budget of 20\n",
      "Processing task: www.walmart.com_www.pricedekho.com.csv, initial shape: 9554\n",
      "Task www.walmart.com_www.pricedekho.com.csv labeled with a budget of 20\n",
      "Processing task: www.walmart.com_www.cambuy.com.au.csv, initial shape: 3777\n",
      "Task www.walmart.com_www.cambuy.com.au.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.mypriceindia.com.csv, initial shape: 92\n",
      "Task www.canon-europe.com_www.mypriceindia.com.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.pcconnection.com.csv, initial shape: 52\n",
      "Task www.canon-europe.com_www.pcconnection.com.csv labeled with a budget of 20\n",
      "Processing task: www.price-hunt.com_cammarkt.com.csv, initial shape: 161\n",
      "Task www.price-hunt.com_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.wexphotographic.com_www.eglobalcentral.co.uk.csv, initial shape: 178\n",
      "Task www.wexphotographic.com_www.eglobalcentral.co.uk.csv labeled with a budget of 20\n",
      "Processing task: buy.net_www.canon-europe.com.csv, initial shape: 94\n",
      "Task buy.net_www.canon-europe.com.csv labeled with a budget of 20\n",
      "Processing task: www.mypriceindia.com_www.price-hunt.com.csv, initial shape: 413\n",
      "Task www.mypriceindia.com_www.price-hunt.com.csv labeled with a budget of 20\n",
      "Processing task: www.wexphotographic.com_www.henrys.com.csv, initial shape: 223\n",
      "Task www.wexphotographic.com_www.henrys.com.csv labeled with a budget of 20\n",
      "Processing task: www.walmart.com_www.shopmania.in.csv, initial shape: 14845\n",
      "Task www.walmart.com_www.shopmania.in.csv labeled with a budget of 20\n",
      "Processing task: www.buzzillions.com_www.gosale.com.csv, initial shape: 1732\n",
      "Task www.buzzillions.com_www.gosale.com.csv labeled with a budget of 20\n",
      "Processing task: www.ebay.com_cammarkt.com.csv, initial shape: 11769\n",
      "Task www.ebay.com_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.wexphotographic.com.csv, initial shape: 26\n",
      "Task www.canon-europe.com_www.wexphotographic.com.csv labeled with a budget of 20\n",
      "Processing task: www.wexphotographic.com_cammarkt.com.csv, initial shape: 45\n",
      "Task www.wexphotographic.com_cammarkt.com.csv labeled with a budget of 20\n",
      "Processing task: www.canon-europe.com_www.ilgs.net.csv, initial shape: 45\n",
      "Task www.canon-europe.com_www.ilgs.net.csv labeled with a budget of 20\n",
      "Processing task: www.ilgs.net_www.wexphotographic.com.csv, initial shape: 144\n",
      "Task www.ilgs.net_www.wexphotographic.com.csv labeled with a budget of 20\n",
      "Elapsed time for active learning: 5.76 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record the start time for active learning\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply active learning to label the selected tasks (uncomment when ready)\n",
    "labeled_tasks = label_linkage_tasks(\n",
    "     selected_tasks, RECORD_LINKAGE_TASKS_PATH, linkage_tasks_general_df,\n",
    "     min_budget=ACTIVE_LEARNING_ITERATION_BUDGET, total_budget=ACTIVE_LEARNING_TOTAL_BUDGET, labeled_tasks_dir=LABELED_RECORD_LINKAGE_TASKS_PATH,active_learning_strategy = ACTIVE_LEARNING_ALGORITHM,\n",
    "     relevant_columns = RELEVANT_COLUMNS_IN_ACTIVE_LEARNING\n",
    ")\n",
    "\n",
    "# Calculate and print the elapsed time for active learning\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time for active learning: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6928980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model on the selected labeled task which is: www.ebay.com_www.priceme.co.nz.csv\n",
      "Inference on the coresponding tasks!\n",
      "Train the model on the selected labeled task which is: www.buzzillions.com_www.mypriceindia.com.csv\n",
      "Inference on the coresponding tasks!\n",
      "Train the model on the selected labeled task which is: www.ukdigitalcameras.co.uk_www.shopmania.in.csv\n",
      "Inference on the coresponding tasks!\n",
      "Train the model on the selected labeled task which is: www.buzzillions.com_www.ebay.com.csv\n",
      "Inference on the coresponding tasks!\n",
      "Train the model on the selected labeled task which is: www.ebay.com_www.gosale.com.csv\n",
      "Inference on the coresponding tasks!\n",
      "Elapsed time for active learning: 3.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record the start time for active learning\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the main transfer learning process on the selected tasks\n",
    "transfear_learning_process_main(\n",
    "    selected_tasks, LABELED_RECORD_LINKAGE_TASKS_PATH,\n",
    "    RECORD_LINKAGE_TASKS_PATH,RELEVANT_COLUMNS_IN_ACTIVE_LEARNING\n",
    ")\n",
    "\n",
    "# Calculate and print the elapsed time for active learning\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time for active learning: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cd7f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: 378760\n",
      "F1 Score: 0.9339\n",
      "Precision: 0.9792\n",
      "Recall: 0.8926\n",
      "Elapsed time for active learning: 1.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record the start time for active learning\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the performance of the labeled tasks\n",
    "evaluate_predictions(LABELED_RECORD_LINKAGE_TASKS_PATH)\n",
    "\n",
    "# Calculate and print the elapsed time for active learning\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time for active learning: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bde61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ead60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b90ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af7796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a26e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f88c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a18e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f0659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd671e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208f3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b767078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d601cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qatarNER",
   "language": "python",
   "name": "qatarner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
